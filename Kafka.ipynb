{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09cfa59",
   "metadata": {},
   "source": [
    "Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafef42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "message = {\"msg\": \"Hello Kafka\"}\n",
    "producer.send('weather_stream', message)\n",
    "producer.flush()\n",
    "print(\"Message envoyé au topic weather_stream\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e78f61",
   "metadata": {},
   "source": [
    "Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import sys\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    topic = sys.argv[1]\n",
    "else:\n",
    "    topic = 'weather_stream'\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    topic,\n",
    "    bootstrap_servers='localhost:9092',            \n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(f\"Lecture des messages du topic {topic}\")\n",
    "\n",
    "for message in consumer:\n",
    "    print(\"Message reçu:\", message.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dda5e8",
   "metadata": {},
   "source": [
    "Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_weather(lat, lon):\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def run_producer(lat, lon):\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='localhost:9092',\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    topic = 'weather_stream'\n",
    "\n",
    "    print(f\"Streaming météo pour latitude={lat}, longitude={lon} vers le topic {topic}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            weather_data = get_weather(lat, lon)\n",
    "            current_weather = weather_data.get('current_weather', {})\n",
    "            if current_weather:\n",
    "                message = {\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'weather': current_weather\n",
    "                }\n",
    "                producer.send(topic, message)\n",
    "                producer.flush()\n",
    "                print(\"Message envoyé:\", message)\n",
    "            else:\n",
    "                print(\"Pas de données météo reçues\")\n",
    "            time.sleep(60)  \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Arrêt du producteur\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage : python current_weather_producer.py <latitude> <longitude>\")\n",
    "        sys.exit(1)\n",
    "    lat = float(sys.argv[1])\n",
    "    lon = float(sys.argv[2])\n",
    "    run_producer(lat, lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91a0e0",
   "metadata": {},
   "source": [
    "Exercice 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, current_timestamp, expr, when\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeatherStreamTransform\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"weather\", StructType([\n",
    "        StructField(\"temperature\", DoubleType()),\n",
    "        StructField(\"windspeed\", DoubleType()),\n",
    "        StructField(\"time\", StringType()) \n",
    "    ]))\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"weather_stream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_json = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "df_parsed = df_json.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_weather = df_parsed.select(\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\"),\n",
    "    expr(\"cast(weather.temperature as double)\").alias(\"temperature\"),\n",
    "    expr(\"cast(weather.windspeed as double)\").alias(\"windspeed\"),\n",
    "    expr(\"cast(weather.time as timestamp)\").alias(\"event_time\")\n",
    ").withColumn(\"event_time\", col(\"event_time\")) \\\n",
    " .withColumn(\"temperature\", col(\"temperature\")) \\\n",
    " .withColumn(\"windspeed\", col(\"windspeed\")) \\\n",
    " .withColumn(\"wind_alert_level\", when(col(\"windspeed\") < 10, \"level_0\")\n",
    "                .when((col(\"windspeed\") >= 10) & (col(\"windspeed\") <= 20), \"level_1\")\n",
    "                .otherwise(\"level_2\")) \\\n",
    " .withColumn(\"heat_alert_level\", when(col(\"temperature\") < 25, \"level_0\")\n",
    "                .when((col(\"temperature\") >= 25) & (col(\"temperature\") <= 35), \"level_1\")\n",
    "                .otherwise(\"level_2\"))\n",
    "\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "df_out = df_weather.select(to_json(struct(\n",
    "    \"event_time\",\n",
    "    \"temperature\",\n",
    "    \"windspeed\",\n",
    "    \"wind_alert_level\",\n",
    "    \"heat_alert_level\"\n",
    ")).alias(\"value\"))\n",
    "\n",
    "query = df_out.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"weather_transformed\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark_checkpoint_weather_transform\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadafbc",
   "metadata": {},
   "source": [
    "Exercice 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118563f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, count, avg, min, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherAggregates\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"wind_alert_level\", StringType()),\n",
    "    StructField(\"heat_alert_level\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"weather_transformed\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_json = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "df_parsed = df_json.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "windowed_df = df_parsed.withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\", \"1 minute\"),\n",
    "        col(\"wind_alert_level\"),\n",
    "        col(\"heat_alert_level\"),\n",
    "        col(\"city\"),\n",
    "        col(\"country\")\n",
    "    ).agg(\n",
    "        count(\"*\").alias(\"count_alerts\"),\n",
    "        avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "        min(\"temperature\").alias(\"min_temperature\"),\n",
    "        max(\"temperature\").alias(\"max_temperature\")\n",
    "    )\n",
    "\n",
    "query = windowed_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01169a25",
   "metadata": {},
   "source": [
    "Exercice 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "def geocode_city(city_name):\n",
    "    url = f\"https://geocoding-api.open-meteo.com/v1/search?name={city_name}&count=1\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    results = response.json().get(\"results\")\n",
    "    if not results:\n",
    "        raise ValueError(f\"Aucune correspondance trouvée pour la ville '{city_name}'\")\n",
    "    data = results[0]\n",
    "    return {\n",
    "        \"latitude\": data[\"latitude\"],\n",
    "        \"longitude\": data[\"longitude\"],\n",
    "        \"city\": data[\"name\"],\n",
    "        \"country\": data.get(\"country\", \"Unknown\")\n",
    "    }\n",
    "\n",
    "def get_weather(lat, lon):\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def run_producer(city_name):\n",
    "    coords = geocode_city(city_name)\n",
    "    lat = coords[\"latitude\"]\n",
    "    lon = coords[\"longitude\"]\n",
    "    city = coords[\"city\"]\n",
    "    country = coords[\"country\"]\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='localhost:9092',\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    topic = 'weather_stream'\n",
    "\n",
    "    print(f\"Streaming météo pour {city}, {country} (lat={lat}, lon={lon}) vers le topic {topic}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            weather_data = get_weather(lat, lon)\n",
    "            current_weather = weather_data.get('current_weather', {})\n",
    "            if current_weather:\n",
    "                message = {\n",
    "                    'city': city,\n",
    "                    'country': country,\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'weather': current_weather\n",
    "                }\n",
    "                producer.send(topic, message)\n",
    "                producer.flush()\n",
    "                print(\"Message envoyé:\", message)\n",
    "            else:\n",
    "                print(\"Pas de données météo reçues\")\n",
    "            time.sleep(60)  \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Arrêt du producteur\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage : python current_weather_producer.py <nom_ville>\")\n",
    "        sys.exit(1)\n",
    "    city_name = sys.argv[1]\n",
    "    run_producer(city_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ed9b9",
   "metadata": {},
   "source": [
    "Exercice 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b03ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"KafkaToHDFS\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"wind_alert_level\", StringType()),\n",
    "    StructField(\"heat_alert_level\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"weather_transformed\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_json = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "df_parsed = df_json.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "hdfs_base_path = \"/hdfs-data\"\n",
    "\n",
    "query = df_parsed.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", hdfs_base_path) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark_checkpoint_kafka_hdfs\") \\\n",
    "    .partitionBy(\"country\", \"city\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9a398",
   "metadata": {},
   "source": [
    "Exercice 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, count, avg, min, max\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherLogsVisualization\").getOrCreate()\n",
    "\n",
    "df_logs = spark.read.json(\"/hdfs-data/**/**/*.json\")\n",
    "\n",
    "df = df_logs.withColumn(\"event_time\", to_timestamp(\"event_time\"))\n",
    "\n",
    "temp_time_df = df.select(\"event_time\", \"temperature\").orderBy(\"event_time\")\n",
    "pandas_temp_time = temp_time_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(pandas_temp_time[\"event_time\"], pandas_temp_time[\"temperature\"], label=\"Température (°C)\")\n",
    "plt.title(\"Évolution de la température au fil du temps\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.ylabel(\"Température\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "windspeed_time_df = df.select(\"event_time\", \"windspeed\").orderBy(\"event_time\")\n",
    "pandas_windspeed_time = windspeed_time_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(pandas_windspeed_time[\"event_time\"], pandas_windspeed_time[\"windspeed\"], label=\"Vitesse du vent (m/s)\", color=\"orange\")\n",
    "plt.title(\"Évolution de la vitesse du vent au fil du temps\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.ylabel(\"Vitesse du vent\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "alerts_count_df = df.groupBy(\"wind_alert_level\", \"heat_alert_level\").count()\n",
    "alerts_count_pd = alerts_count_df.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "alerts_count_pd.plot.bar(x=\"wind_alert_level\", y=\"count\", ax=ax, label=\"Alertes Vent\", color='blue', alpha=0.7)\n",
    "alerts_count_pd.plot.bar(x=\"heat_alert_level\", y=\"count\", ax=ax, label=\"Alertes Chaleur\", color='red', alpha=0.7)\n",
    "plt.title(\"Nombre d’alertes vent et chaleur par niveau\")\n",
    "plt.xlabel(\"Niveau d'alerte\")\n",
    "plt.ylabel(\"Nombre d'alertes\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if \"country\" in df.columns and \"weather_code\" in df.columns:\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number, desc\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"country\").orderBy(desc(\"count\"))\n",
    "    weather_freq_df = df.groupBy(\"country\", \"weather_code\").count()\n",
    "    weather_ranked = weather_freq_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "    top_weather_per_country = weather_ranked.filter(col(\"rank\") == 1).toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.bar(top_weather_per_country[\"country\"], top_weather_per_country[\"count\"])\n",
    "    plt.title(\"Code météo le plus fréquent par pays\")\n",
    "    plt.xlabel(\"Pays\")\n",
    "    plt.ylabel(\"Nombre d'occurrences\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Colonnes 'country' ou 'weather_code' absentes. Skipping code météo fréquent par pays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffbe65f",
   "metadata": {},
   "source": [
    "Exercice 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30948f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def fetch_weather_history(lat, lon, start_date, end_date):\n",
    "    url = (\n",
    "        f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "        f\"latitude={lat}&longitude={lon}\"\n",
    "        f\"&start_date={start_date}&end_date={end_date}\"\n",
    "        f\"&hourly=temperature_2m,windspeed_10m\"\n",
    "        f\"&timezone=Europe/Paris\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "topic = \"weather_history_raw\"\n",
    "city = \"Paris\"\n",
    "country = \"France\"\n",
    "latitude = 48.8566\n",
    "longitude = 2.3522\n",
    "\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "print(f\"Récupération historique météo pour {city} ({latitude}, {longitude}) de {start_date} à {end_date}\")\n",
    "\n",
    "data = fetch_weather_history(latitude, longitude, start_date, end_date)\n",
    "\n",
    "hourly_data = data.get(\"hourly\", {})\n",
    "temp_list = hourly_data.get(\"temperature_2m\", [])\n",
    "wind_list = hourly_data.get(\"windspeed_10m\", [])\n",
    "times = hourly_data.get(\"time\", [])\n",
    "\n",
    "for t, temp, wind in zip(times, temp_list, wind_list):\n",
    "    message = {\n",
    "        \"city\": city,\n",
    "        \"country\": country,\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"timestamp\": t,\n",
    "        \"temperature\": temp,\n",
    "        \"windspeed\": wind\n",
    "    }\n",
    "    producer.send(topic, message)\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "producer.flush()\n",
    "print(\"Tous les messages historiques envoyés dans Kafka\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HistoryKafkaToHDFS\").getOrCreate()\n",
    "\n",
    "df_raw = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType())\n",
    "])\n",
    "\n",
    "df_parsed = df_raw.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_final = df_parsed.withColumn(\"event_time\", to_timestamp(\"timestamp\")).drop(\"timestamp\")\n",
    "\n",
    "hdfs_path = f\"/hdfsdata/{country}/{city}/weather_history_raw\"\n",
    "\n",
    "df_final.write.mode(\"overwrite\").partitionBy(\"country\", \"city\").json(hdfs_path)\n",
    "\n",
    "print(f\"Données historiques sauvegardées dans HDFS sous {hdfs_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
