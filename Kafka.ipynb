{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "try:\n",
    "    s.connect(('localhost', 9092))\n",
    "    print(\"Connexion réussie à localhost:9092\")\n",
    "except Exception as e:\n",
    "    print(\"Erreur de connexion:\", e)\n",
    "finally:\n",
    "    s.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cfa59",
   "metadata": {},
   "source": [
    "Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafef42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:29092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "\n",
    "message = {\"msg\": \"Hello Kafka\"}\n",
    "producer.send('weather_stream', message)\n",
    "producer.flush()\n",
    "print(\"Message envoyé au topic weather_stream\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e78f61",
   "metadata": {},
   "source": [
    "Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "topic = 'weather_stream'\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    topic,\n",
    "    bootstrap_servers='localhost:29092', \n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(f\"Lecture des messages du topic {topic}\")\n",
    "\n",
    "for message in consumer:\n",
    "    print(\"Message reçu:\", message.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dda5e8",
   "metadata": {},
   "source": [
    "Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_weather(lat, lon):\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def run_producer(lat, lon):\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='localhost:29092',\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    topic = 'weather_stream'\n",
    "\n",
    "    print(f\"Streaming météo pour latitude={lat}, longitude={lon} vers le topic {topic}\")\n",
    "\n",
    "    for _ in range(3):  \n",
    "        weather_data = get_weather(lat, lon)\n",
    "        current_weather = weather_data.get('current_weather', {})\n",
    "        if current_weather:\n",
    "            message = {\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'weather': current_weather\n",
    "            }\n",
    "            producer.send(topic, message)\n",
    "            producer.flush()\n",
    "            print(\"Message envoyé:\", message)\n",
    "        else:\n",
    "            print(\"Pas de données météo reçues\")\n",
    "        time.sleep(10)\n",
    "\n",
    "latitude = 48.8566\n",
    "longitude = 2.3522\n",
    "\n",
    "run_producer(latitude, longitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91a0e0",
   "metadata": {},
   "source": [
    "Exercice 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fa870",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, when, to_json, struct\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSpark4\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Scala version: {spark.sparkContext._jvm.scala.util.Properties.versionString()}\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"weather\", StructType([\n",
    "        StructField(\"temperature\", DoubleType()),\n",
    "        StructField(\"windspeed\", DoubleType()),\n",
    "        StructField(\"time\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"weather_stream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_json = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "df_parsed = df_json.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_weather = df_parsed.select(\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\"),\n",
    "    expr(\"cast(weather.temperature as double)\").alias(\"temperature\"),\n",
    "    expr(\"cast(weather.windspeed as double)\").alias(\"windspeed\"),\n",
    "    expr(\"cast(weather.time as timestamp)\").alias(\"event_time\")\n",
    ").withColumn(\"wind_alert_level\",\n",
    "             when(col(\"windspeed\") < 10, \"level_0\")\n",
    "             .when((col(\"windspeed\") >= 10) & (col(\"windspeed\") <= 20), \"level_1\")\n",
    "             .otherwise(\"level_2\")) \\\n",
    " .withColumn(\"heat_alert_level\",\n",
    "             when(col(\"temperature\") < 25, \"level_0\")\n",
    "             .when((col(\"temperature\") >= 25) & (col(\"temperature\") <= 35), \"level_1\")\n",
    "             .otherwise(\"level_2\"))\n",
    "\n",
    "df_out = df_weather.select(to_json(struct(\n",
    "    \"event_time\",\n",
    "    \"temperature\",\n",
    "    \"windspeed\",\n",
    "    \"wind_alert_level\",\n",
    "    \"heat_alert_level\"\n",
    ")).alias(\"value\"))\n",
    "\n",
    "query = df_out.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"weather_transformed\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark_checkpoint_weather_transform\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadafbc",
   "metadata": {},
   "source": [
    "Exercice 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1319a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, count, avg, min, max\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exo5_AgrégatsSeparés\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"wind_alert_level\", StringType()),\n",
    "    StructField(\"heat_alert_level\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"weather_transformed\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_json = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "df_parsed = df_json.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "window_duration = \"5 minutes\"\n",
    "slide_duration = \"1 minute\"\n",
    "\n",
    "with_watermark = df_parsed.withWatermark(\"event_time\", \"10 minutes\")\n",
    "\n",
    "alerts = with_watermark.filter(\n",
    "    (col(\"wind_alert_level\").isin(\"level_1\", \"level_2\")) |\n",
    "    (col(\"heat_alert_level\").isin(\"level_1\", \"level_2\"))\n",
    ")\n",
    "\n",
    "alerts_count = alerts.groupBy(\n",
    "    window(\"event_time\", window_duration, slide_duration),\n",
    "    \"city\", \"country\"\n",
    ").agg(count(\"*\").alias(\"alert_count\"))\n",
    "\n",
    "temp_stats = with_watermark.groupBy(\n",
    "    window(\"event_time\", window_duration, slide_duration),\n",
    "    \"city\", \"country\"\n",
    ").agg(\n",
    "    avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "    min(\"temperature\").alias(\"min_temperature\"),\n",
    "    max(\"temperature\").alias(\"max_temperature\"),\n",
    ")\n",
    "\n",
    "alerts_query = alerts_count.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "temp_query = temp_stats.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "alerts_query.awaitTermination()\n",
    "temp_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01169a25",
   "metadata": {},
   "source": [
    "Exercice 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_weather(lat, lon):\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def run_producer(lat, lon):\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='localhost:29092',\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    topic = 'weather_stream'\n",
    "\n",
    "    print(f\"Streaming météo pour latitude={lat}, longitude={lon} vers le topic {topic}\")\n",
    "\n",
    "    for _ in range(3):  \n",
    "        weather_data = get_weather(lat, lon)\n",
    "        current_weather = weather_data.get('current_weather', {})\n",
    "        if current_weather:\n",
    "            message = {\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'weather': current_weather\n",
    "            }\n",
    "            producer.send(topic, message)\n",
    "            producer.flush()\n",
    "            print(\"Message envoyé:\", message)\n",
    "        else:\n",
    "            print(\"Pas de données météo reçues\")\n",
    "        time.sleep(10)\n",
    "\n",
    "latitude = 48.8566\n",
    "longitude = 2.3522\n",
    "\n",
    "run_producer(latitude, longitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb2f60",
   "metadata": {},
   "source": [
    "Exercice 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleKafkaToLocal\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"wind_alert_level\", StringType()),\n",
    "    StructField(\"heat_alert_level\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"weather_transformed\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_json = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "df_parsed = df_json.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "alerts = df_parsed.filter(\n",
    "    (col(\"wind_alert_level\").isin(\"level_1\", \"level_2\"))\n",
    "    | (col(\"heat_alert_level\").isin(\"level_1\", \"level_2\"))\n",
    ")\n",
    "\n",
    "query = alerts.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"./alerts_data\") \\\n",
    "    .option(\"checkpointLocation\", \"./alerts_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536552aa",
   "metadata": {},
   "source": [
    "Exercice 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.read_json(\"./alerts_data/alerts_test.json\")  \n",
    "\n",
    "pdf[\"event_time\"] = pd.to_datetime(pdf[\"event_time\"])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"wind_alert_level\", StringType()),\n",
    "    StructField(\"heat_alert_level\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(pdf, schema=schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(pdf[\"event_time\"], pdf[\"temperature\"])\n",
    "plt.title(\"Évolution température\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.ylabel(\"Température (°C)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(pdf[\"event_time\"], pdf[\"windspeed\"])\n",
    "plt.title(\"Évolution vitesse du vent\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.ylabel(\"Vitesse du vent (m/s)\")\n",
    "plt.show()\n",
    "\n",
    "alerts_count_wind = pdf.groupby(\"wind_alert_level\").size()\n",
    "alerts_count_heat = pdf.groupby(\"heat_alert_level\").size()\n",
    "\n",
    "plt.bar(alerts_count_wind.index, alerts_count_wind.values, alpha=0.7, label=\"Vent\")\n",
    "plt.bar(alerts_count_heat.index, alerts_count_heat.values, alpha=0.4, label=\"Chaleur\")\n",
    "plt.title(\"Nombre d'alertes vent et chaleur par niveau\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755df74",
   "metadata": {},
   "source": [
    "Exercice 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def fetch_weather_history_yearly(start_year, end_year):\n",
    "    all_records = []\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        if year == datetime.today().year:\n",
    "            end_date = today_str \n",
    "        else:\n",
    "            end_date = f\"{year}-12-31\"\n",
    "        print(f\"Fetching data for {start_date} to {end_date}...\")\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": 48.8566,\n",
    "            \"longitude\": 2.3522,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \"temperature_2m,windspeed_10m\",\n",
    "            \"timezone\": \"Europe/Paris\"\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        for dt, temp, wind in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"], data[\"hourly\"][\"windspeed_10m\"]):\n",
    "            all_records.append({\n",
    "                \"event_time\": dt,\n",
    "                \"temperature\": temp,\n",
    "                \"windspeed\": wind,\n",
    "                \"city\": \"Paris\",\n",
    "                \"country\": \"France\"\n",
    "            })\n",
    "    return all_records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddbb1d",
   "metadata": {},
   "source": [
    "Exercice 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, row_number\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exo10_RecordsClimatiques_Local\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", StringType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"precipitation\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json(\"./alerts_data/alerts_test.json\")\n",
    "df = df.withColumn(\"date\", to_date(col(\"event_time\")))\n",
    "\n",
    "w = Window.partitionBy(\"city\")\n",
    "\n",
    "max_temp_per_day = df.groupBy(\"city\", \"date\").max(\"temperature\").withColumnRenamed(\"max(temperature)\", \"max_temp\")\n",
    "max_temp_record = max_temp_per_day.withColumn(\"rn\", row_number().over(w.orderBy(col(\"max_temp\").desc()))) \\\n",
    "    .filter(col(\"rn\") == 1).select(\"city\", \"date\", \"max_temp\")\n",
    "\n",
    "min_temp_per_day = df.groupBy(\"city\", \"date\").min(\"temperature\").withColumnRenamed(\"min(temperature)\", \"min_temp\")\n",
    "min_temp_record = min_temp_per_day.withColumn(\"rn\", row_number().over(w.orderBy(col(\"min_temp\").asc()))) \\\n",
    "    .filter(col(\"rn\") == 1).select(\"city\", \"date\", \"min_temp\")\n",
    "\n",
    "max_wind_record = df.withColumn(\"rn\", row_number().over(w.orderBy(col(\"windspeed\").desc()))) \\\n",
    "    .filter(col(\"rn\") == 1).select(\"city\", \"event_time\", \"windspeed\")\n",
    "\n",
    "if \"precipitation\" in df.columns:\n",
    "    rain_per_day = df.groupBy(\"city\", \"date\").sum(\"precipitation\").withColumnRenamed(\"sum(precipitation)\", \"total_precip\")\n",
    "    rain_record = rain_per_day.withColumn(\"rn\", row_number().over(w.orderBy(col(\"total_precip\").desc()))) \\\n",
    "        .filter(col(\"rn\") == 1).select(\"city\", \"date\", \"total_precip\")\n",
    "else:\n",
    "    rain_record = None\n",
    "\n",
    "def convert_dates(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_dates(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_dates(v) for v in obj]\n",
    "    elif isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "        return obj.isoformat()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "records = {\n",
    "    \"max_temp\": max_temp_record.toPandas().to_dict(orient=\"records\"),\n",
    "    \"min_temp\": min_temp_record.toPandas().to_dict(orient=\"records\"),\n",
    "    \"max_wind\": max_wind_record.toPandas().to_dict(orient=\"records\"),\n",
    "    \"max_precip\": rain_record.toPandas().to_dict(orient=\"records\") if rain_record is not None else []\n",
    "}\n",
    "records_json_ready = convert_dates(records)\n",
    "\n",
    "with open(\"./alerts_data/weather_records.json\", \"w\") as f:\n",
    "    json.dump(records_json_ready, f, indent=2)\n",
    "\n",
    "print(\"Records calculés et écrits dans alerts_data/weather_records.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203b116",
   "metadata": {},
   "source": [
    "Exercice 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, year, avg, count, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exo11_ClimatologieUrbaine_Local\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", StringType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"windspeed\", DoubleType()),\n",
    "    StructField(\"wind_alert_level\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json(\"./alerts_data/alerts_test.json\")\n",
    "\n",
    "df = df.withColumn(\"month\", month(col(\"event_time\"))).withColumn(\"year\", year(col(\"event_time\")))\n",
    "\n",
    "seasonal_profile = df.groupBy(\"country\", \"city\", \"month\").agg(\n",
    "    avg(\"temperature\").alias(\"temperature_month_avg\"),\n",
    "    avg(\"windspeed\").alias(\"windspeed_month_avg\"),\n",
    "    (count(when(col(\"wind_alert_level\").isin(\"level_1\", \"level_2\"), 1)) / count(\"*\")).alias(\"alert_probability\")\n",
    ")\n",
    "\n",
    "profile_dict = seasonal_profile.toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "with open(\"./alerts_data/seasonal_profile.json\", \"w\") as f:\n",
    "    json.dump(profile_dict, f, indent=2)\n",
    "\n",
    "print(\"Profils saisonniers calculés et écrits dans alerts_data/seasonal_profile.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83787122",
   "metadata": {},
   "source": [
    "Exercice 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, year, avg, stddev, min, max, expr, count, when, percentile_approx\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exo12_ValidationEnrichissement\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"month\", DoubleType()),  \n",
    "    StructField(\"temperature_month_avg\", DoubleType()),\n",
    "    StructField(\"windspeed_month_avg\", DoubleType()),\n",
    "    StructField(\"alert_probability\", DoubleType())\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json(\"./alerts_data/seasonal_profile.json\")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "grouped = df.groupBy(\"country\", \"city\").agg(countDistinct(\"month\").alias(\"months_count\"))\n",
    "incomplete_profiles = grouped.filter(col(\"months_count\") < 12).collect()\n",
    "\n",
    "if incomplete_profiles:\n",
    "    print(\"Profils incomplets détectés:\")\n",
    "    for r in incomplete_profiles:\n",
    "        print(f\"{r['city']}, {r['country']} : {r['months_count']} mois détectés\")\n",
    "else:\n",
    "    print(\"Tous les profils ont 12 mois complets\")\n",
    "\n",
    "\n",
    "df_enriched = df.groupBy(\"country\", \"city\", \"month\").agg(\n",
    "    avg(\"temperature_month_avg\").alias(\"avg_temp\"),\n",
    "    stddev(\"temperature_month_avg\").alias(\"std_temp\"),\n",
    "    min(\"temperature_month_avg\").alias(\"min_temp\"),\n",
    "    max(\"temperature_month_avg\").alias(\"max_temp\"),\n",
    "    expr(\"percentile_approx(temperature_month_avg, 0.5)\").alias(\"median_temp\"),\n",
    "    expr(\"percentile_approx(temperature_month_avg, array(0.25, 0.75))\").alias(\"q25_q75_temp\"),\n",
    "\n",
    "    avg(\"windspeed_month_avg\").alias(\"avg_wind\"),\n",
    "    stddev(\"windspeed_month_avg\").alias(\"std_wind\"),\n",
    "    min(\"windspeed_month_avg\").alias(\"min_wind\"),\n",
    "    max(\"windspeed_month_avg\").alias(\"max_wind\"),\n",
    "    expr(\"percentile_approx(windspeed_month_avg, 0.5)\").alias(\"median_wind\"),\n",
    "    expr(\"percentile_approx(windspeed_month_avg, array(0.25, 0.75))\").alias(\"q25_q75_wind\"),\n",
    "\n",
    "    avg(\"alert_probability\").alias(\"avg_alert_prob\")\n",
    ")\n",
    "\n",
    "df_valid = df_enriched.filter(\n",
    "    (col(\"min_temp\") >= -50) & (col(\"max_temp\") <= 60) &\n",
    "    (col(\"min_wind\") >= 0) & (col(\"max_wind\") <= 60)\n",
    ")\n",
    "\n",
    "\n",
    "profile_enriched_pd = df_valid.toPandas()\n",
    "\n",
    "output_dir = \"./alerts_data/seasonal_profile_enriched\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, \"profile_enriched.json\"), \"w\") as f:\n",
    "    json.dump(profile_enriched_pd.to_dict(orient=\"records\"), f, indent=2)\n",
    "\n",
    "print(\"Profils saisonniers validés, enrichis et sauvegardés en local sous\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8808229",
   "metadata": {},
   "source": [
    "Exercice 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, year, avg, stddev, min, max, count, when, expr, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exo13_DetectionAnomalies\").getOrCreate()\n",
    "\n",
    "profile_schema = StructType([\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"month\", DoubleType()),\n",
    "    StructField(\"avg_temp\", DoubleType()),\n",
    "    StructField(\"std_temp\", DoubleType()),\n",
    "    StructField(\"min_temp\", DoubleType()),\n",
    "    StructField(\"max_temp\", DoubleType()),\n",
    "    StructField(\"median_temp\", DoubleType()),\n",
    "    StructField(\"q25_q75_temp\", StringType()), \n",
    "    StructField(\"avg_wind\", DoubleType()),\n",
    "    StructField(\"std_wind\", DoubleType()),\n",
    "    StructField(\"min_wind\", DoubleType()),\n",
    "    StructField(\"max_wind\", DoubleType()),\n",
    "    StructField(\"median_wind\", DoubleType()),\n",
    "    StructField(\"q25_q75_wind\", StringType()),\n",
    "    StructField(\"avg_alert_prob\", DoubleType())\n",
    "])\n",
    "\n",
    "profiles_df = spark.read.schema(profile_schema).json(\"./alerts_data/seasonal_profile_enriched/profile_enriched.json\")\n",
    "\n",
    "realtime = {\n",
    "    \"event_time\": \"2025-09-23T15:00:00\",\n",
    "    \"city\": \"Paris\",\n",
    "    \"country\": \"France\",\n",
    "    \"temperature\": 30,\n",
    "    \"windspeed\": 20,\n",
    "    \"wind_alert_level\": \"level_2\"\n",
    "}\n",
    "\n",
    "realtime_month = datetime.datetime.strptime(realtime[\"event_time\"], \"%Y-%m-%dT%H:%M:%S\").month\n",
    "\n",
    "profiles_pd = profiles_df.toPandas()\n",
    "profile_row = profiles_pd[\n",
    "    (profiles_pd[\"country\"] == realtime[\"country\"]) &\n",
    "    (profiles_pd[\"city\"] == realtime[\"city\"]) &\n",
    "    (profiles_pd[\"month\"] == realtime_month)\n",
    "]\n",
    "if profile_row.empty:\n",
    "    print(\"Pas de profil historique pour cette clé\")\n",
    "else:\n",
    "    profile = profile_row.iloc[0]\n",
    "    anomalies = []\n",
    "    if abs(realtime[\"temperature\"] - profile[\"avg_temp\"]) > 5:\n",
    "        anomalies.append({\n",
    "            \"variable\": \"temperature\",\n",
    "            \"observed_value\": realtime[\"temperature\"],\n",
    "            \"expected_value\": profile[\"avg_temp\"],\n",
    "            \"anomaly_type\": \"temperature_anomaly\"\n",
    "        })\n",
    "    wind_thresh = profile[\"avg_wind\"] + 2 * profile[\"std_wind\"] if not pd.isna(profile[\"std_wind\"]) else None\n",
    "    if wind_thresh and realtime[\"windspeed\"] > wind_thresh:\n",
    "        anomalies.append({\n",
    "            \"variable\": \"windspeed\",\n",
    "            \"observed_value\": realtime[\"windspeed\"],\n",
    "            \"expected_value\": profile[\"avg_wind\"],\n",
    "            \"anomaly_type\": \"windspeed_anomaly\"\n",
    "        })\n",
    "   \n",
    "\n",
    "    event_anomalies = []\n",
    "    for an in anomalies:\n",
    "        event_anomalies.append({\n",
    "            \"city\": realtime[\"city\"],\n",
    "            \"country\": realtime[\"country\"],\n",
    "            \"event_time\": realtime[\"event_time\"],\n",
    "            \"variable\": an[\"variable\"],\n",
    "            \"observed_value\": an[\"observed_value\"],\n",
    "            \"expected_value\": an[\"expected_value\"],\n",
    "            \"anomaly_type\": an[\"anomaly_type\"]\n",
    "        })\n",
    "\n",
    "    if event_anomalies:\n",
    "        out_dir = f\"./hdfs-data/{realtime['country']}/{realtime['city']}/anomalies/{datetime.datetime.now().year}/{datetime.datetime.now().month:02d}\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        fname = f\"{out_dir}/anomalies_{realtime['event_time'].replace(':','')}.json\"\n",
    "        with open(fname, \"w\") as f:\n",
    "            json.dump(event_anomalies, f, indent=2)\n",
    "        print(f\"Anomalies détectées et sauvegardées dans {fname}\")\n",
    "    else:\n",
    "        print(\"Aucune anomalie détectée\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb2360",
   "metadata": {},
   "source": [
    "Exercice 14 - dashboard.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
